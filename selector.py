import asyncio
import pandas as pd
import numpy as np
import torch
import os
import sys
from typing import Dict, Union, List, Optional
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from llama_index.llms.ollama import Ollama
import sentence_transformers
from sentence_transformers import SentenceTransformer, util

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥ä½ çš„æ–‡æ¡£å¤„ç†æ¨¡å—
from src.utils.logger import setup_logger
from src.pipeline.document_pipeline import DocumentPipeline
from src.utils.config import get_config

@dataclass
class AnalysisResult:
    """æ•°æ®åˆ†æç»“æœ"""
    data: pd.DataFrame
    query: str
    source: str

@dataclass
class DocQnAResult:
    """æ–‡æ¡£é—®ç­”ç»“æœ"""
    context: str
    metadata: Dict
    query: str
    source: str

@dataclass
class ProcessingResult:
    """æ–‡æ¡£å¤„ç†ç»“æœ"""
    success: bool
    message: str
    data: Optional[Union[pd.DataFrame, List[Dict]]] = None

class EnhancedIntentProcessor:
    """å¢å¼ºçš„æ„å›¾å¤„ç†å™¨ï¼Œé›†æˆæ–‡æ¡£å¤„ç†æµæ°´çº¿"""
    
    def __init__(self, embedding_model: SentenceTransformer):
        self.llm = Ollama(model="llama3.1", request_timeout=30.0)
        self.embedding_model = embedding_model
        
        # åˆå§‹åŒ–æ—¥å¿—å’Œé…ç½®
        setup_logger()
        self.config = get_config()
        
        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†æµæ°´çº¿
        self.document_pipeline = DocumentPipeline()
        
        print("âœ… å¢å¼ºæ„å›¾å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    async def process_query(
        self,
        query: str,
        data: Optional[pd.DataFrame] = None,
        file_path: Optional[str] = None
    ) -> Union[AnalysisResult, DocQnAResult, ProcessingResult]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            data: å·²æœ‰çš„æ•°æ®ï¼ˆå¯é€‰ï¼‰
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            å¤„ç†ç»“æœ
        """
        
        # å¦‚æœæœ‰æ–‡ä»¶è·¯å¾„ï¼Œå…ˆå¤„ç†æ–‡ä»¶
        if file_path:
            processing_result = await self._process_document(file_path)
            if not processing_result.success:
                return processing_result
            
            # å¦‚æœå¤„ç†çš„æ˜¯è¡¨æ ¼æ–‡ä»¶ï¼Œè¿”å›AnalysisResult
            if file_path.lower().endswith(('.csv', '.xlsx', '.xls')):
                return AnalysisResult(
                    data=processing_result.data,
                    query=query,
                    source=file_path
                )
        
        # å¦‚æœæœ‰å‘é‡åŒ–æ•°æ®ï¼Œè¿›è¡Œæ–‡æ¡£é—®ç­”
        if data is not None and 'vector' in data.columns and 'text_chunk' in data.columns:
            return self._find_best_match(query, data)
        
        # å¦‚æœæœ‰è¡¨æ ¼æ•°æ®ï¼Œè¿›è¡Œæ•°æ®åˆ†æ
        elif data is not None:
            return AnalysisResult(
                data=data,
                query=query,
                source="provided_data"
            )
        
        # é»˜è®¤è¿”å›å¤„ç†ç»“æœ
        return ProcessingResult(
            success=True,
            message="æŸ¥è¯¢å·²æ¥æ”¶ï¼Œè¯·æä¾›æ•°æ®æˆ–æ–‡ä»¶è¿›è¡Œå¤„ç†",
            data=None
        )

    async def _process_document(self, file_path: str) -> ProcessingResult:
        """å¤„ç†æ–‡æ¡£æ–‡ä»¶"""
        try:
            print(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}")
            
            # ä½¿ç”¨æ–‡æ¡£å¤„ç†æµæ°´çº¿
            result = self.document_pipeline.process_document(file_path)
            
            if result['success']:
                print(f"âœ… æ–‡æ¡£å¤„ç†æˆåŠŸ: {file_path}")
                return ProcessingResult(
                    success=True,
                    message=f"æ–‡æ¡£ {Path(file_path).name} å¤„ç†æˆåŠŸ",
                    data=result.get('data')
                )
            else:
                print(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {file_path}")
                return ProcessingResult(
                    success=False,
                    message=f"æ–‡æ¡£å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                )
                
        except Exception as e:
            print(f"âŒ æ–‡æ¡£å¤„ç†å¼‚å¸¸: {str(e)}")
            return ProcessingResult(
                success=False,
                message=f"æ–‡æ¡£å¤„ç†å¼‚å¸¸: {str(e)}"
            )

    def _find_best_match(self, query: str, data: pd.DataFrame) -> DocQnAResult:
        """åœ¨å‘é‡åŒ–æ•°æ®ä¸­æŸ¥æ‰¾æœ€ä½³åŒ¹é…"""
        try:
            query_vector = self.embedding_model.encode(query, convert_to_tensor=True)
            corpus_vectors_np = np.stack(data['vector'].values)
            corpus_vectors = torch.from_numpy(corpus_vectors_np).to(query_vector.device)
            cosine_scores = util.cos_sim(query_vector, corpus_vectors)[0]
            best_match_idx = np.argmax(cosine_scores.cpu().numpy())
            best_match_score = cosine_scores[best_match_idx]
            best_match_row = data.iloc[best_match_idx]

            matched_context = best_match_row['text_chunk']
            
            metadata = {
                "source": best_match_row.get('source', 'Unknown'),
                "chunk_index": int(best_match_row.name),
                "similarity_score": float(best_match_score),
                "full_content": matched_context,
                "processed_at": datetime.now().isoformat(),
            }

            return DocQnAResult(
                context=matched_context,
                metadata=metadata,
                query=query,
                source=best_match_row.get('source', 'Unknown')
            )
        except Exception as e:
            print(f"âŒ å‘é‡åŒ¹é…å¤±è´¥: {str(e)}")
            return DocQnAResult(
                context="åŒ¹é…å¤±è´¥",
                metadata={"error": str(e)},
                query=query,
                source="error"
            )

    async def search_documents(self, query: str, top_k: int = 5) -> List[Dict]:
        """æœç´¢å·²å¤„ç†çš„æ–‡æ¡£"""
        try:
            results = self.document_pipeline.search_documents(query, top_k)
            return results
        except Exception as e:
            print(f"âŒ æ–‡æ¡£æœç´¢å¤±è´¥: {str(e)}")
            return []

    def get_statistics(self) -> Dict:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        try:
            return self.document_pipeline.get_statistics()
        except Exception as e:
            print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {"error": str(e)}

# è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºä¸€ä¸ªç»è¿‡å‘é‡åŒ–çš„çŸ¥è¯†åº“DataFrame
def create_vectorized_kb(model: SentenceTransformer) -> pd.DataFrame:
    """åˆ›å»ºç¤ºä¾‹å‘é‡åŒ–çŸ¥è¯†åº“"""
    chunks = [
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ã€‚",
        "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒå’Œè¯­éŸ³è¯†åˆ«ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚",
        "å¯æŒç»­å‘å±•éœ€è¦å¹³è¡¡ç»æµã€ç¤¾ä¼šå’Œç¯å¢ƒä¸‰è€…ã€‚",
        "ä¼ä¸šç¤¾ä¼šè´£ä»»æ˜¯å…¬å¸å¯¹ç¤¾ä¼šè´¡çŒ®çš„ä½“ç°ã€‚",
    ]
    
    print("æ­£åœ¨ä½¿ç”¨æ¨¡å‹å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œè¯·ç¨å€™...")
    vectors = model.encode(chunks)
    print("ç¼–ç å®Œæˆã€‚")
    
    df = pd.DataFrame({
        "text_chunk": chunks,
        "vector": list(vectors),
        "source": ["doc_ai.txt", "doc_ai.txt", "doc_sustainability.txt", "doc_sustainability.txt"]
    })
    
    return df

async def main():
    """ä¸»å‡½æ•° - æµ‹è¯•é›†æˆåçš„åŠŸèƒ½"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆæ–‡æ¡£é—®ç­”å’Œæ•°æ®åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    
    print("æ­£åœ¨åŠ è½½æ–‡æœ¬ç¼–ç æ¨¡å‹...")
    embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    print("æ¨¡å‹åŠ è½½å®Œæ¯•ã€‚")
    
    processor = EnhancedIntentProcessor(embedding_model=embedding_model)

    print("\n\n----------- 1. æ–‡æ¡£é—®ç­”ä»»åŠ¡æµ‹è¯• -----------")
    vector_kb_df = create_vectorized_kb(embedding_model)
    print("\n[è¾“å…¥çŸ¥è¯†åº“]")
    print(vector_kb_df[['text_chunk', 'source']])
    print("-------------------------------------------")

    query_doc = "AIçš„å…³é”®æ˜¯ä»€ä¹ˆï¼Ÿ"
    doc_qa_result = await processor.process_query(query_doc, vector_kb_df)
    
    print("\n[è¾“å‡ºç»“æœ]")
    print(f"å¤„ç†ç»“æœç±»å‹: {type(doc_qa_result)}")
    if isinstance(doc_qa_result, DocQnAResult):
        print(f"åŸå§‹é—®é¢˜: {doc_qa_result.query}")
        print(f"åŒ¹é…åˆ°çš„ä¸Šä¸‹æ–‡: {doc_qa_result.context}")
        print(f"æ¥æº: {doc_qa_result.source}")
        print(f"è¯¦ç»†å…ƒæ•°æ®: {doc_qa_result.metadata}")

    print("\n\n----------- 2. æ•°æ®åˆ†æä»»åŠ¡æµ‹è¯• -----------")
    
    df_sales = pd.DataFrame({
        "äº§å“ID": ["A101", "A102", "B201"],
        "æ—¥æœŸ": pd.to_datetime(["2025-08-18", "2025-08-18", "2025-08-19"]),
        "é”€å”®é¢": [150.0, 200.5, 75.0]
    })
    
    print("\n[è¾“å…¥æ•°æ®]")
    print(df_sales)
    print("-------------------------------------------")
    
    query_analysis = "8æœˆ18æ—¥å“ªæ¬¾äº§å“é”€é‡æœ€é«˜ï¼Ÿ"
    analysis_result = await processor.process_query(query_analysis, df_sales)
    
    print(f"\n[è¾“å‡ºç»“æœ]")
    print(f"å¤„ç†ç»“æœç±»å‹: {type(analysis_result)}")
    if isinstance(analysis_result, AnalysisResult):
        print(f"åŸå§‹é—®é¢˜: {analysis_result.query}")
        print(f"æ•°æ®ï¼š{analysis_result.data}")
        print(f"æ¥æº: {analysis_result.source}")

    print("\n\n----------- 3. æ–‡æ¡£å¤„ç†åŠŸèƒ½æµ‹è¯• -----------")
    print("ğŸ’¡ æç¤ºï¼šä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æµ‹è¯•æ–‡æ¡£å¤„ç†åŠŸèƒ½ï¼š")
    print("   - å¤„ç†PDFæ–‡ä»¶: await processor.process_query('æŸ¥è¯¢å†…å®¹', file_path='path/to/file.pdf')")
    print("   - å¤„ç†è¡¨æ ¼æ–‡ä»¶: await processor.process_query('æŸ¥è¯¢å†…å®¹', file_path='path/to/file.csv')")
    print("   - æœç´¢æ–‡æ¡£: await processor.search_documents('æœç´¢å…³é”®è¯')")
    print("   - è·å–ç»Ÿè®¡: processor.get_statistics()")

if __name__ == "__main__":
    asyncio.run(main())