import asyncio
import pandas as pd
import numpy as np
import torch
import os
import sys
from typing import Dict, Union, List, Optional
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from llama_index.llms.ollama import Ollama
import sentence_transformers
from sentence_transformers import SentenceTransformer, util

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥ä½ çš„æ–‡æ¡£å¤„ç†æ¨¡å—
from src.utils.logger import setup_logger
from src.pipeline.document_pipeline import DocumentPipeline
from src.utils.config import get_config

from src.llm_generate.data_analyze2 import analyze_data
from src.llm_generate.doc_qa import doc_qa

os.environ["http_proxy"] = "http://127.0.0.1:7890"
os.environ["https_proxy"] = "http://127.0.0.1:7890"
os.environ['SILICONFLOW_API_KEY'] = 'sk-ealsjxzyweovkgxqlshbuophkshxjwaiawrldqnvlhzhsatd'
@dataclass
class AnalysisResult:
    """æ•°æ®åˆ†æç»“æœ"""
    data: pd.DataFrame
    query: str
    source: str

@dataclass
class DocQnAResult:
    """æ–‡æ¡£é—®ç­”ç»“æœ"""
    context: str
    metadata: Dict
    query: str
    source: str

@dataclass
class ProcessingResult:
    """æ–‡æ¡£å¤„ç†ç»“æœ"""
    success: bool
    message: str
    data: Optional[Union[pd.DataFrame, List[Dict]]] = None

class EnhancedIntentProcessor:
    """å¢å¼ºçš„æ„å›¾å¤„ç†å™¨ï¼Œé›†æˆæ–‡æ¡£å¤„ç†ä¸æ„å›¾è¯†åˆ«åŠŸèƒ½"""
    
    def __init__(self, embedding_model: SentenceTransformer):
        self.llm = Ollama(model="qwen3:8b", request_timeout=30.0)
        self.embedding_model = embedding_model
        
        # åˆå§‹åŒ–æ—¥å¿—å’Œé…ç½®
        setup_logger()
        self.config = get_config()
        
        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†æµæ°´çº¿
        self.document_pipeline = DocumentPipeline()
        
        print("âœ… å¢å¼ºæ„å›¾å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    async def process_query(
        self,
        query: str,
        data: Optional[pd.DataFrame] = None,
        file_path: Optional[str] = None
    ) -> Union[AnalysisResult, DocQnAResult, ProcessingResult]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            data: å·²æœ‰çš„æ•°æ®ï¼ˆå¯é€‰ï¼‰
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            å¤„ç†ç»“æœ
        """
        
        # å¦‚æœæœ‰æ–‡ä»¶è·¯å¾„ï¼Œå…ˆå¤„ç†æ–‡ä»¶
        if file_path:
            processing_result = await self._process_document(file_path)
            if not processing_result.success:
                return processing_result
            
            # å¦‚æœå¤„ç†çš„æ˜¯è¡¨æ ¼æ–‡ä»¶ï¼Œè¿”å›AnalysisResult
            if file_path.lower().endswith(('.csv', '.xlsx', '.xls')):
                return AnalysisResult(
                    data=processing_result.data,
                    query=query,
                    source=file_path
                )
        
        # å¦‚æœæœ‰å‘é‡åŒ–æ•°æ®ï¼Œè¿›è¡Œæ–‡æ¡£é—®ç­”
        if data is not None and 'vector' in data.columns and 'text_chunk' in data.columns:
            best = self._find_best_match(query, data)
            def to_snippets(x: Union[DocQnAResult, List[DocQnAResult]]) -> List[Dict[str, str]]:
                if isinstance(x, list):
                    return [{"source": i.source, "text": i.context} for i in x]
                else:
                    return [{"source": x.source, "text": x.context}]
            snippets = to_snippets(best)
            
            doc_qa_result = doc_qa(query, snippets)
            return doc_qa_result
        
        # å¦‚æœæœ‰è¡¨æ ¼æ•°æ®ï¼Œè¿›è¡Œæ•°æ®åˆ†æ
        elif data is not None:
            # return AnalysisResult(
            #     data=data,
            #     query=query,
            #     source="provided_data"
            # )
            return analyze_data(query, data)
        
        # é»˜è®¤è¿”å›å¤„ç†ç»“æœ
        return ProcessingResult(
            success=True,
            message="æŸ¥è¯¢å·²æ¥æ”¶ï¼Œè¯·æä¾›æ•°æ®æˆ–æ–‡ä»¶è¿›è¡Œå¤„ç†",
            data=None
        )

    async def _process_document(self, file_path: str) -> ProcessingResult:
        """å¤„ç†æ–‡æ¡£æ–‡ä»¶"""
        try:
            print(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}")
            
            # ä½¿ç”¨æ–‡æ¡£å¤„ç†æµæ°´çº¿
            result = self.document_pipeline.process_document(file_path)
            
            if result['success']:
                print(f"âœ… æ–‡æ¡£å¤„ç†æˆåŠŸ: {file_path}")
                return ProcessingResult(
                    success=True,
                    message=f"æ–‡æ¡£ {Path(file_path).name} å¤„ç†æˆåŠŸ",
                    data=result.get('data')
                )
            else:
                print(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {file_path}")
                return ProcessingResult(
                    success=False,
                    message=f"æ–‡æ¡£å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                )
                
        except Exception as e:
            print(f"âŒ æ–‡æ¡£å¤„ç†å¼‚å¸¸: {str(e)}")
            return ProcessingResult(
                success=False,
                message=f"æ–‡æ¡£å¤„ç†å¼‚å¸¸: {str(e)}"
            )

    def _find_best_match(self, query: str, data: pd.DataFrame) -> DocQnAResult:
        """åœ¨å‘é‡åŒ–æ•°æ®ä¸­æŸ¥æ‰¾æœ€ä½³åŒ¹é…"""
        try:
            query_vector = self.embedding_model.encode(query, convert_to_tensor=True)
            corpus_vectors_np = np.stack(data['vector'].values)
            corpus_vectors = torch.from_numpy(corpus_vectors_np).to(query_vector.device)
            cosine_scores = util.cos_sim(query_vector, corpus_vectors)[0]
            best_match_idx = np.argmax(cosine_scores.cpu().numpy())
            best_match_score = cosine_scores[best_match_idx]
            best_match_row = data.iloc[best_match_idx]

            matched_context = best_match_row['text_chunk']
            
            metadata = {
                "source": best_match_row.get('source', 'Unknown'),
                "chunk_index": int(best_match_row.name),
                "similarity_score": float(best_match_score),
                "full_content": matched_context,
                "processed_at": datetime.now().isoformat(),
            }

            return DocQnAResult(
                context=matched_context,
                metadata=metadata,
                query=query,
                source=best_match_row.get('source', 'Unknown')
            )
        except Exception as e:
            print(f"âŒ å‘é‡åŒ¹é…å¤±è´¥: {str(e)}")
            return DocQnAResult(
                context="åŒ¹é…å¤±è´¥",
                metadata={"error": str(e)},
                query=query,
                source="error"
            )

    async def search_documents(self, query: str, top_k: int = 5) -> List[Dict]:
        """æœç´¢å·²å¤„ç†çš„æ–‡æ¡£"""
        try:
            results = self.document_pipeline.search_documents(query, top_k)
            return results
        except Exception as e:
            print(f"âŒ æ–‡æ¡£æœç´¢å¤±è´¥: {str(e)}")
            return []

    def get_statistics(self) -> Dict:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        try:
            return self.document_pipeline.get_statistics()
        except Exception as e:
            print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {"error": str(e)}

# è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºä¸€ä¸ªç»è¿‡å‘é‡åŒ–çš„çŸ¥è¯†åº“DataFrame
def create_vectorized_kb(model: SentenceTransformer) -> pd.DataFrame:
    """åˆ›å»ºç¤ºä¾‹å‘é‡åŒ–çŸ¥è¯†åº“"""
    chunks = [
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ã€‚",
        "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒå’Œè¯­éŸ³è¯†åˆ«ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚",
        "å¯æŒç»­å‘å±•éœ€è¦å¹³è¡¡ç»æµã€ç¤¾ä¼šå’Œç¯å¢ƒä¸‰è€…ã€‚",
        "ä¼ä¸šç¤¾ä¼šè´£ä»»æ˜¯å…¬å¸å¯¹ç¤¾ä¼šè´¡çŒ®çš„ä½“ç°ã€‚",
    ]
    
    print("æ­£åœ¨ä½¿ç”¨æ¨¡å‹å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œè¯·ç¨å€™...")
    vectors = model.encode(chunks)
    print("ç¼–ç å®Œæˆã€‚")
    
    df = pd.DataFrame({
        "text_chunk": chunks,
        "vector": list(vectors),
        "source": ["doc_ai.txt", "doc_ai.txt", "doc_sustainability.txt", "doc_sustainability.txt"]
    })
    
    return df

async def main():
    """ä¸»å‡½æ•° - æµ‹è¯•é›†æˆåçš„åŠŸèƒ½"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆæ–‡æ¡£é—®ç­”å’Œæ•°æ®åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    
    print("æ­£åœ¨åŠ è½½æ–‡æœ¬ç¼–ç æ¨¡å‹...")
    embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    print("æ¨¡å‹åŠ è½½å®Œæ¯•ã€‚")
    
    processor = EnhancedIntentProcessor(embedding_model=embedding_model)
    vector_kb_df = create_vectorized_kb(embedding_model)

    print("\n\n----------- 1. æ–‡æ¡£é—®ç­”ä»»åŠ¡æµ‹è¯• -----------")
    query_doc = "AIçš„å…³é”®æ˜¯ä»€ä¹ˆï¼Ÿ"
    
    doc_qa_result = await processor.process_query(query_doc, vector_kb_df)
    # doc_qa_result = await processor.process_query(query=query_doc, file_path='./data/documents/æ·±åº¦å­¦ä¹ ï¼ˆèŠ±ä¹¦ï¼‰.pdf')
    
    print("\n[è¾“å‡ºç»“æœ]")
    print(doc_qa_result)
    # print(f"å¤„ç†ç»“æœç±»å‹: {type(doc_qa_result)}")
    # if isinstance(doc_qa_result, DocQnAResult):
    #     print(f"åŸå§‹é—®é¢˜: {doc_qa_result.query}")
    #     print(f"åŒ¹é…åˆ°çš„ä¸Šä¸‹æ–‡: {doc_qa_result.context}")
    #     print(f"æ¥æº: {doc_qa_result.source}")
    #     print(f"è¯¦ç»†å…ƒæ•°æ®: {doc_qa_result.metadata}")

    print("\n\n----------- 2. æ•°æ®åˆ†æä»»åŠ¡æµ‹è¯• -----------")
    
    df_sales = pd.read_csv("./data/tables/order_details.csv")
    print("\n[è¾“å…¥æ•°æ®ç¤ºä¾‹]")
    print(df_sales.head(10))
    print("-------------------------------------------")
    query_analysis = "ä¸åŒç§ç±»å•†å“çš„å”®å–æ•°é‡ã€å”®å–æ€»é¢ã€åˆ©æ¶¦ä»¥åŠåˆ©æ¶¦å æ¯”ï¼Ÿ"
    analysis_result = await processor.process_query(query_analysis, df_sales)
    print(f"\n[è¾“å‡ºç»“æœ]")
    print(analysis_result)

    print("\n\n----------- 3. æ–‡æ¡£å¤„ç†åŠŸèƒ½æµ‹è¯• -----------")
    print("ğŸ’¡ æç¤ºï¼šä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æµ‹è¯•æ–‡æ¡£å¤„ç†åŠŸèƒ½ï¼š")
    print("   - å¤„ç†PDFæ–‡ä»¶: await processor.process_query('æŸ¥è¯¢å†…å®¹', file_path='path/to/file.pdf')")
    print("   - å¤„ç†è¡¨æ ¼æ–‡ä»¶: await processor.process_query('æŸ¥è¯¢å†…å®¹', file_path='path/to/file.csv')")
    print("   - æœç´¢æ–‡æ¡£: await processor.search_documents('æœç´¢å…³é”®è¯')")
    print("   - è·å–ç»Ÿè®¡: processor.get_statistics()")

if __name__ == "__main__":
    asyncio.run(main())